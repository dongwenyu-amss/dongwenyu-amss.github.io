[{"content":"Finite Element\r定义如下的有限元空间\n$$\nV_h := \\{\\mu_h\\in P_{rd}(G_h):(rot_h\\mu_h,\\ita_h)-(\\mu_h,curl\\ita_h)=0,\\forall\\ita_h\\in V_{h0}^{CR}, and (div_h\\mu_h,\\tau_h)-(\\mu_h,\\nabla_h\\tau_h)=0,\\forall\\tau_h\\in V_h^1\\}\r$$\n生成三角剖分\n","date":"2025-12-26T00:00:00Z","permalink":"https://dongwenyu-amss.github.io/p/a-primal-finite-element-scheme-of-the-hodge-laplace-problem/","title":"A Primal Finite Element Scheme of The Hodge Laplace Problem"},{"content":" ","date":"2025-05-09T00:00:00Z","image":"https://dongwenyu-amss.github.io/p/2025%E5%B9%B45%E6%9C%889%E6%97%A5-%E9%A2%90%E5%92%8C%E5%9B%AD/1_hu6269834795887137379.jpg","permalink":"https://dongwenyu-amss.github.io/p/2025%E5%B9%B45%E6%9C%889%E6%97%A5-%E9%A2%90%E5%92%8C%E5%9B%AD/","title":"2025年5月9日 颐和园"},{"content":" ","date":"2025-05-01T00:00:00Z","image":"https://dongwenyu-amss.github.io/p/2025%E5%B9%B45%E6%9C%881%E6%97%A5-%E5%8C%97%E4%BA%AC%E8%A5%BF%E5%B1%B1%E5%9B%BD%E5%AE%B6%E6%A3%AE%E6%9E%97%E5%85%AC%E5%9B%AD/2_hu1211311782989021181.jpg","permalink":"https://dongwenyu-amss.github.io/p/2025%E5%B9%B45%E6%9C%881%E6%97%A5-%E5%8C%97%E4%BA%AC%E8%A5%BF%E5%B1%B1%E5%9B%BD%E5%AE%B6%E6%A3%AE%E6%9E%97%E5%85%AC%E5%9B%AD/","title":"2025年5月1日 北京西山国家森林公园"},{"content":"Question\r考虑二维圆形区域上的poisson问题：\n$$\r-\\triangle u = f\r$$边界条件选取dirichlet条件。\n其变分形式为\n$$\r\\min_{u\\in H} I(u)\r$$其中\n$$\rI(u)=\\int_{\\Omega}(\\frac{1}{2}|\\nabla u(x)|^2-f(x)u(x))dx\r$$采用DeepRitz方法求解的python代码如下：\n首先导入用到的package：\n1 2 3 4 5 6 7 8 import numpy as np import math, torch, generateData, time import torch.nn.functional as F from torch.optim.lr_scheduler import MultiStepLR, StepLR import torch.nn as nn import matplotlib.pyplot as plt import sys, os import writeSolution 定义神经网络\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class RitzNet(torch.nn.Module): def __init__(self, params): super(RitzNet, self).__init__() self.params = params self.linearIn = nn.Linear(self.params[\u0026#34;d\u0026#34;], self.params[\u0026#34;width\u0026#34;]) ##Input Layer self.linear = nn.ModuleList() ##Hidden Layer for _ in range(params[\u0026#34;depth\u0026#34;]): self.linear.append(nn.Linear(self.params[\u0026#34;width\u0026#34;], self.params[\u0026#34;width\u0026#34;])) self.linearOut = nn.Linear(self.params[\u0026#34;width\u0026#34;], self.params[\u0026#34;dd\u0026#34;]) ##Output Layer def forward(self, x): x = torch.tanh(self.linearIn(x)) # Match dimension for layer in self.linear: x_temp = torch.tanh(layer(x)) x = x_temp return self.linearOut(x) 预训练\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def preTrain(model,device,params,preOptimizer,preScheduler,fun): model.train() file = open(\u0026#34;lossData.txt\u0026#34;,\u0026#34;w\u0026#34;) for step in range(params[\u0026#34;preStep\u0026#34;]): # The volume integral data = torch.from_numpy(generateData.sampleFromDisk(params[\u0026#34;radius\u0026#34;],params[\u0026#34;bodyBatch\u0026#34;])).float().to(device) output = model(data) target = fun(params[\u0026#34;radius\u0026#34;],data) loss = output-target loss = torch.mean(loss*loss)*math.pi*params[\u0026#34;radius\u0026#34;]**2 if step%params[\u0026#34;writeStep\u0026#34;] == params[\u0026#34;writeStep\u0026#34;]-1: with torch.no_grad(): ref = exact(params[\u0026#34;radius\u0026#34;],data) error = errorFun(output,ref,params) # print(\u0026#34;Loss at Step %s is %s.\u0026#34;%(step+1,loss.item())) print(\u0026#34;Error at Step %s is %s.\u0026#34;%(step+1,error)) file.write(str(step+1)+\u0026#34; \u0026#34;+str(error)+\u0026#34;\\n\u0026#34;) model.zero_grad() loss.backward() # Update the weights. preOptimizer.step() # preScheduler.step() 训练Ritz神经网络\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def train(model,device,params,optimizer,scheduler): model.train() data1 = torch.from_numpy(generateData.sampleFromDisk(params[\u0026#34;radius\u0026#34;],params[\u0026#34;bodyBatch\u0026#34;])).float().to(device) data1.requires_grad = True data2 = torch.from_numpy(generateData.sampleFromSurface(params[\u0026#34;radius\u0026#34;],params[\u0026#34;bdryBatch\u0026#34;])).float().to(device) for step in range(params[\u0026#34;trainStep\u0026#34;]-params[\u0026#34;preStep\u0026#34;]): output1 = model(data1) model.zero_grad() dfdx = torch.autograd.grad(output1,data1,grad_outputs=torch.ones_like(output1),retain_graph=True,create_graph=True,only_inputs=True)[0] # Loss function 1 fTerm = ffun(data1).to(device) loss1 = torch.mean(0.5*torch.sum(dfdx*dfdx,1).unsqueeze(1)-fTerm*output1) * math.pi*params[\u0026#34;radius\u0026#34;]**2 # Loss function 2 output2 = model(data2) target2 = exact(params[\u0026#34;radius\u0026#34;],data2) loss2 = torch.mean((output2-target2)*(output2-target2) * params[\u0026#34;penalty\u0026#34;] * 2*math.pi*params[\u0026#34;radius\u0026#34;]) loss = loss1+loss2 if step%params[\u0026#34;writeStep\u0026#34;] == params[\u0026#34;writeStep\u0026#34;]-1: with torch.no_grad(): target = exact(params[\u0026#34;radius\u0026#34;],data1) error = errorFun(output1,target,params) # print(\u0026#34;Loss at Step %s is %s.\u0026#34;%(step+params[\u0026#34;preStep\u0026#34;]+1,loss.item())) print(\u0026#34;Error at Step %s is %s.\u0026#34;%(step+params[\u0026#34;preStep\u0026#34;]+1,error)) file = open(\u0026#34;lossData.txt\u0026#34;,\u0026#34;a\u0026#34;) file.write(str(step+params[\u0026#34;preStep\u0026#34;]+1)+\u0026#34; \u0026#34;+str(error)+\u0026#34;\\n\u0026#34;) if step%params[\u0026#34;sampleStep\u0026#34;] == params[\u0026#34;sampleStep\u0026#34;]-1: data1 = torch.from_numpy(generateData.sampleFromDisk(params[\u0026#34;radius\u0026#34;],params[\u0026#34;bodyBatch\u0026#34;])).float().to(device) data1.requires_grad = True data2 = torch.from_numpy(generateData.sampleFromSurface(params[\u0026#34;radius\u0026#34;],params[\u0026#34;bdryBatch\u0026#34;])).float().to(device) if 10*(step+1)%params[\u0026#34;trainStep\u0026#34;] == 0: print(\u0026#34;%s%% finished...\u0026#34;%(100*(step+1)//params[\u0026#34;trainStep\u0026#34;])) loss.backward() optimizer.step() scheduler.step() 计算误差的 $L^2$ 范数\n1 2 3 4 5 6 def errorFun(output,target,params): error = output-target error = math.sqrt(torch.mean(error*error)*math.pi*params[\u0026#34;radius\u0026#34;]**2) # Calculate the L2 norm error. ref = math.sqrt(torch.mean(target*target)*math.pi*params[\u0026#34;radius\u0026#34;]**2) return error/ref 精确解及源项\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def test(model,device,params): numQuad = params[\u0026#34;numQuad\u0026#34;] data = torch.from_numpy(generateData.sampleFromDisk(1,numQuad)).float().to(device) output = model(data) target = exact(params[\u0026#34;radius\u0026#34;],data).to(device) error = output-target error = math.sqrt(torch.mean(error*error)*math.pi*params[\u0026#34;radius\u0026#34;]**2) # Calculate the L2 norm error. ref = math.sqrt(torch.mean(target*target)*math.pi*params[\u0026#34;radius\u0026#34;]**2) return error/ref def ffun(data): # f = 4 return 4.0*torch.ones([data.shape[0],1],dtype=torch.float) def exact(r,data): # f = 4 ==\u0026gt; u = r^2-x^2-y^2 output = r**2-torch.sum(data*data,dim=1) return output.unsqueeze(1) def rough(r,data): # A rough guess output = r**2-r*torch.sum(data*data,dim=1)**0.5 return output.unsqueeze(1) def count_parameters(model): return sum(p.numel() for p in model.parameters()) 模型参数及计算\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def main(): # Parameters # torch.manual_seed(21) device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) params = dict() params[\u0026#34;radius\u0026#34;] = 1 params[\u0026#34;d\u0026#34;] = 2 # 2D params[\u0026#34;dd\u0026#34;] = 1 # Scalar field params[\u0026#34;bodyBatch\u0026#34;] = 1024 # Batch size params[\u0026#34;bdryBatch\u0026#34;] = 1024 # Batch size for the boundary integral params[\u0026#34;lr\u0026#34;] = 0.01 # Learning rate params[\u0026#34;preLr\u0026#34;] = 0.01 # Learning rate (Pre-training) params[\u0026#34;width\u0026#34;] = 8 # Width of layers params[\u0026#34;depth\u0026#34;] = 2 # Depth of the network: depth+2 params[\u0026#34;numQuad\u0026#34;] = 40000 # Number of quadrature points for testing params[\u0026#34;trainStep\u0026#34;] = 50000 params[\u0026#34;penalty\u0026#34;] = 500 params[\u0026#34;preStep\u0026#34;] = 0 params[\u0026#34;writeStep\u0026#34;] = 50 params[\u0026#34;sampleStep\u0026#34;] = 10 params[\u0026#34;step_size\u0026#34;] = 5000 params[\u0026#34;gamma\u0026#34;] = 0.5 params[\u0026#34;decay\u0026#34;] = 0.00001 startTime = time.time() model = RitzNet(params).to(device) print(\u0026#34;Generating network costs %s seconds.\u0026#34;%(time.time()-startTime)) preOptimizer = torch.optim.Adam(model.parameters(),lr=params[\u0026#34;preLr\u0026#34;]) optimizer = torch.optim.Adam(model.parameters(),lr=params[\u0026#34;lr\u0026#34;],weight_decay=params[\u0026#34;decay\u0026#34;]) scheduler = StepLR(optimizer,step_size=params[\u0026#34;step_size\u0026#34;],gamma=params[\u0026#34;gamma\u0026#34;]) startTime = time.time() preTrain(model,device,params,preOptimizer,None,rough) train(model,device,params,optimizer,scheduler) print(\u0026#34;Training costs %s seconds.\u0026#34;%(time.time()-startTime)) model.eval() testError = test(model,device,params) print(\u0026#34;The test error (of the last model) is %s.\u0026#34;%testError) print(\u0026#34;The number of parameters is %s,\u0026#34;%count_parameters(model)) torch.save(model.state_dict(),\u0026#34;last_model.pt\u0026#34;) pltResult(model,device,100,params) 绘制结果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def pltResult(model,device,nSample,params): rList = np.linspace(0,params[\u0026#34;radius\u0026#34;],nSample) thetaList = np.linspace(0,math.pi*2,nSample) xx = np.zeros([nSample,nSample]) yy = np.zeros([nSample,nSample]) zz = np.zeros([nSample,nSample]) for i in range(nSample): for j in range(nSample): xx[i,j] = rList[i]*math.cos(thetaList[j]) yy[i,j] = rList[i]*math.sin(thetaList[j]) coord = np.array([xx[i,j],yy[i,j]]) zz[i,j] = model(torch.from_numpy(coord).float().to(device)).item() # zz[i,j] = params[\u0026#34;radius\u0026#34;]**2-xx[i,j]**2-yy[i,j]**2 # Plot the exact solution. file = open(\u0026#34;nSample.txt\u0026#34;,\u0026#34;w\u0026#34;) file.write(str(nSample)) file = open(\u0026#34;Data.txt\u0026#34;,\u0026#34;w\u0026#34;) writeSolution.write(xx,yy,zz,nSample,file) edgeList = [[params[\u0026#34;radius\u0026#34;]*math.cos(i),params[\u0026#34;radius\u0026#34;]*math.sin(i)] for i in thetaList] writeSolution.writeBoundary(edgeList) if __name__==\u0026#34;__main__\u0026#34;: main() generateData.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 import numpy as np import math import matplotlib.pyplot as plt # Sample points in a disk def sampleFromDisk(r,n): \u0026#34;\u0026#34;\u0026#34; r -- radius; n -- number of samples. \u0026#34;\u0026#34;\u0026#34; array = np.random.rand(2*n,2)*2*r-r array = np.multiply(array.T,(np.linalg.norm(array,2,axis=1)\u0026lt;r)).T array = array[~np.all(array==0, axis=1)] if np.shape(array)[0]\u0026gt;=n: return array[0:n] else: return sampleFromDisk(r,n) def sampleFromDomain(n): # For simplicity, consider a square with a hole. # Square: [-1,1]*[-1,1] # Hole: c = (0.3,0.0), r = 0.3 array = np.zeros([n,2]) c = np.array([0.3,0.0]) r = 0.3 for i in range(n): array[i] = randomPoint(c,r) return array def randomPoint(c,r): point = np.random.rand(2)*2-1 if np.linalg.norm(point-c)\u0026lt;r: return randomPoint(c,r) else: return point def sampleFromBoundary(n): # For simplicity, consider a square with a hole. # Square: [-1,1]*[-1,1] # Hole: c = (0.3,0.0), r = 0.3 c = np.array([0.3,0.0]) r = 0.3 length = 4*2+2*math.pi*r interval1 = np.array([0.0,2.0/length]) interval2 = np.array([2.0/length,4.0/length]) interval3 = np.array([4.0/length,6.0/length]) interval4 = np.array([6.0/length,8.0/length]) interval5 = np.array([8.0/length,1.0]) array = np.zeros([n,2]) for i in range(n): rand0 = np.random.rand() rand1 = np.random.rand() point1 = np.array([rand1*2.0-1.0,-1.0]) point2 = np.array([rand1*2.0-1.0,+1.0]) point3 = np.array([-1.0,rand1*2.0-1.0]) point4 = np.array([+1.0,rand1*2.0-1.0]) point5 = np.array([c[0]+r*math.cos(2*math.pi*rand1),c[1]+r*math.sin(2*math.pi*rand1)]) array[i] = myFun(rand0,interval1)*point1 + myFun(rand0,interval2)*point2 + \\ myFun(rand0,interval3)*point3 + myFun(rand0,interval4)*point4 + \\ myFun(rand0,interval5)*point5 return array def myFun(x,interval): if interval[0] \u0026lt;= x \u0026lt;= interval[1]: return 1.0 else: return 0.0 def sampleFromSurface(r,n): \u0026#34;\u0026#34;\u0026#34; r -- radius; n -- number of samples. \u0026#34;\u0026#34;\u0026#34; array = np.random.normal(size=(n,2)) norm = np.linalg.norm(array,2,axis=1) # print(np.min(norm)) if np.min(norm) == 0: return sampleFromSurface(r,n) else: array = np.multiply(array.T,1/norm).T return array*r # Sample from 10d-ball def sampleFromDisk10(r,n): \u0026#34;\u0026#34;\u0026#34; r -- radius; n -- number of samples. \u0026#34;\u0026#34;\u0026#34; array = np.random.normal(size=(n,10)) norm = np.linalg.norm(array,2,axis=1) # print(np.min(norm)) if np.min(norm) == 0: return sampleFromDisk10(r,n) else: array = np.multiply(array.T,1/norm).T radius = np.random.rand(n,1)**(1/10) array = np.multiply(array,radius) return r*array def sampleFromSurface10(r,n): \u0026#34;\u0026#34;\u0026#34; r -- radius; n -- number of samples. \u0026#34;\u0026#34;\u0026#34; array = np.random.normal(size=(n,10)) norm = np.linalg.norm(array,2,axis=1) # print(np.min(norm)) if np.min(norm) == 0: return sampleFromSurface10(r,n) else: array = np.multiply(array.T,1/norm).T return array*r if __name__ == \u0026#34;__main__\u0026#34;: # array = sampleFromDomain(10000).T # array = sampleFromBoundary(500).T # plt.plot(array[0],array[1],\u0026#39;o\u0026#39;,ls=\u0026#34;None\u0026#34;) # plt.axis(\u0026#34;equal\u0026#34;) # plt.show() pass writeSolution.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import sys, os import numpy as np import math def writeRow(list,file): for i in list: file.write(\u0026#34;%s \u0026#34;%i) file.write(\u0026#34;\\n\u0026#34;) def write(X,Y,Z,nSampling,file): for k1 in range(nSampling): writeRow(X[k1],file) writeRow(Y[k1],file) writeRow(Z[k1],file) def writeBoundary(edgeList,edgeList2 = None): length=[] file=open(\u0026#34;boundaryCoord.txt\u0026#34;,\u0026#34;w\u0026#34;) for i in edgeList: writeRow(i,file) if edgeList2 != None: for i in edgeList2: writeRow(i,file) file=open(\u0026#34;boundaryNumber.txt\u0026#34;,\u0026#34;w\u0026#34;) if edgeList2 == None: length = [len(edgeList)] else: length = [len(edgeList),len(edgeList2)] for i in length: file.write(\u0026#34;%s\\n\u0026#34;%i) if __name__==\u0026#34;__main__\u0026#34;: pass 数值结果：取区域为以原点为中心的单位圆，重复五次DeepRitz方法，得到最终五次loss的平均值随迭代次数变化的曲线图如下：\n数值解的图像：\n","date":"2025-02-17T00:00:00Z","image":"https://dongwenyu-amss.github.io/p/deep-ritz-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/Numerical_hu4737791081319894639.jpg","permalink":"https://dongwenyu-amss.github.io/p/deep-ritz-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/","title":"Deep Ritz 代码实现"},{"content":" ","date":"2025-02-15T00:00:00Z","image":"https://dongwenyu-amss.github.io/p/2025%E5%B9%B42%E6%9C%8815%E6%97%A5-%E5%8A%A8%E7%89%A9%E5%9B%AD/2_hu14698893709018612583.jpg","permalink":"https://dongwenyu-amss.github.io/p/2025%E5%B9%B42%E6%9C%8815%E6%97%A5-%E5%8A%A8%E7%89%A9%E5%9B%AD/","title":"2025年2月15日 动物园"},{"content":"Question\r考虑变分问题：\n$$\r\\min_{u\\in H} I(u)\r$$其中\n$$\rI(u)=\\int_{\\Omega}(\\frac{1}{2}|\\nabla u(x)|^2-f(x)u(x))dx\r$$这里$H$为试验函数空间，$f$为源项。\nIdea\rDeep Ritz方法基于以下思想：\n1.深度神经网络对于试验函数的逼近\n2.对于泛函的数值积分法则\n3.求解最终优化问题的算法\nMethod\rTrial Function\rDeep Ritz方法的核心是一个由深度神经网络定义的非线性变换 $z_{\\theta}(x)\\in R^m$ ，$\\theta$ 为参数。一个神经网络中，每一层都由几个块堆叠而成，每个块中包含两个线性变换，两个激活函数以及一个残差连接，其输入s和输出t都是 $R^m$维向量，第i个块可以表示为：\n$$\rt=f_i(s)=\\phi(W_{i,2}\\cdot\\phi(W_{i,1}s+b_{i,1})+b_{i,2})+s\r$$其中， $W_{i,1},W_{i,2}\\in R^{m\\times m},b_{i,1},b_{i,2}\\in R^m$ 是参数， $\\phi$为激活函数，这里选用\n$$\r\\phi(x)=\\max\\{x^3,0\\}\r$$完整的n层网络可以表示为\n$$\rz_{\\theta}(x)=f_n\\circ\\cdots\\circ f_1(x)\r$$$\\theta$ 代表神经网络中的所有参数。这里注意第一个块的输入x是d维向量，为处理维度差异，若 $d\u0026lt;m$ ，可用0补全x，若 $d\u0026gt;m$ ，可以通过对 $z_{\\theta}(x)$ 作线性变换来得到 $u(x)$:\n$$\ru(x;\\theta)=a\\cdot z_{\\theta}(x)+b\r$$现在我们得到了一个关于 $\\theta$ 的函数，下面考虑极小化泛函问题。\n记\n$$\rg(x;\\theta)=\\frac{1}{2}|\\nabla u(x;\\theta)|^2-f(x)u(x;\\theta)\r$$从而得到优化问题如下：\n$$\r\\min_{\\theta} L(\\theta), L(\\theta)=\\int_{\\Omega}g(x;\\theta)dx\r$$SGD and Quadrature Rule\r在机器学习中，优化问题通常表示为以下形式：\n$$\r\\min_{x\\in R^d}L(\\theta):= \\frac{1}{N}\\sum_{i=1}^N L_i(\\theta)\r$$右端项中，每一项对应一个数据点，N通常很大，对于这个问题，可以选用随机梯度下降法：\n$$\r\\theta^{k+1}=\\theta^k -\\eta\\nabla L_{\\gamma^k}(\\theta^k)\r$$其中 $\\gamma^k$ 是均匀分布在 ${1,2,\\cdots,N}$ 上的独立同分布随机变量。\n在Deep Ritz Method中，若将积分视作连续和，则可以得到类似的优化形式。具体做法为，在SGD的每一步，从 $\\Omega$ 中随机选出一小部分点作为数据点，在每个点使用相同的积分权重。\n综上，Deep Ritz方法的SGD由以下迭代给出：\n$$\r\\theta^{k+1}=\\theta^k-\\eta\\nabla_{\\theta}\\frac{1}{M}\\sum_{j=1}^M g(x_{j,k};\\theta^k)\r$$对于每一步k，$ {x_{j,k}} $ 都是从 $\\Omega$ 中随机选取的一组服从均匀分布的点。\n","date":"2025-02-13T00:00:00Z","permalink":"https://dongwenyu-amss.github.io/p/deep-ritz-method/","title":"Deep Ritz Method"}]