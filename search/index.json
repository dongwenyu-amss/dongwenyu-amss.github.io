[{"content":"Question\r考虑变分问题：\n$$\rmin_{u\\in H} I(u)\r$$其中\n$$\rI(u)=\\int_{\\Omega}(\\frac{1}{2}|\\nabla u(x)|^2-f(x)u(x))dx\r$$这里$H$为试验函数空间，$f$为源项。\nIdea\rDeep Ritz方法基于以下思想：\n1.深度神经网络对于试验函数的逼近\n2.对于泛函的数值积分法则\n3.求解最终优化问题的算法\nMethod\rTrial Function\rDeep Ritz方法的核心是一个由深度神经网络定义的非线性变换 $z_{\\theta}(x)\\in R^m$ ，$\\theta$ 为参数。一个神经网络中，每一层都由几个块堆叠而成，每个块中包含两个线性变换，两个激活韩素以及一个残差连接，其输入s和输出t都是 $R^m$维向量，第i个块可以表示为：\n$$\rt=f_i(s)=\\phi(W_{i,2}\\cdot\\phi(W_{i,1}s+b_{i,1})+b_{i,2})+s\r$$其中， $W_{i,1},W_{i,2}\\in R^{m\\times m},b_{i,1},b_{i,2}\\in R^m$ 是参数， $\\phi$为激活函数，这里选用\n$$\r\\phi(x)=\\max\\{x^3,0\\}\r$$完整的n层网络可以表示为\n$$\rz_{\\theta}(x)=f_n\\circ\\cdots\\circ f_1(x)\r$$$\\theta$ 代表神经网络中的所有参数。这里注意第一个块的输入x是d维向量，为处理维度差异，若 $d\u0026lt;m$ ，可用0补全x，若 $d\u0026gt;m$ ，可以通过对 $z_{\\theta}(x)$ 作线性变换来得到 $u(x)$:\n$$\ru(x;\\theta)=a\\cdot z_{\\theta}(x)+b\r$$现在我们得到了一个关于 $\\theta$ 的函数，下面考虑极小化泛函问题。\n记\n$$\rg(x;\\theta)=\\frac{1}{2}|\\nabla u(x;\\theta)|^2-f(x)u(x;\\theta)\r$$从而得到优化问题如下：\n$$\rmin_{\\theta} L(\\theta), L(\\theta)=\\int_{\\Omega}g(x;\\theta)dx\r$$SGD and Quadrature Rule\r在机器学习中，优化问题通常表示为以下形式：\n$$\rmin_{x\\in R^d}L(\\theta):= \\frac{1}{N}\\sum_{i=1}^N L_i(\\theta)\r$$右端项中，每一项对应一个数据点，N通常很大，对于这个问题，可以选用随机梯度下降法：\n$$\r\\theta^{k+1}=\\theta^k -\\eta\\nabla L_{\\gamma^k}(\\theta^k)\r$$其中 $\\gamma^k$ 是均匀分布在 ${1,2,\\cdots,N}$ 上的独立同分布随机变量。\n在Deep Ritz Method中，若将积分视作连续和，则可以得到类似的优化形式。具体做法为，在SGD的每一步，从 $\\Omega$ 中随机选出一小部分点作为数据点，在每个点使用相同的积分权重。\n综上，Deep Ritz方法的SGD由以下迭代给出：\n$$\r\\theta^{k+1}=\\theta^k-\\eta\\nabla_{\\theta}\\frac{1}{M}\\sum_{j=1}^M g(x_{j,k};\\theta^k)\r$$对于每一步k，$ {x_{j,k}} $ 都是从 $\\Omega$ 中随机选取的一组服从均匀分布的点。\n","date":"2025-02-13T00:00:00Z","permalink":"http://localhost:1313/p/deep-ritz-method/","title":"Deep Ritz Method"}]